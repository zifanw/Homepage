<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Zifan Wang</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script src="js/main.js" type="text/javascript"></script>

   <!-- Fonts -->
   <!-- ADD A GOOGLE FONT HERE IF YOU LIKE -->

    
  </head>
  <body>

    
<div class="color-header">
  
  <div class="container page-header">
  
  <div class="row" id="contact">
    <div class="col-md-2">
      <img class="photo" src="images/photo.jpg" alt="Photo">
    </div>
    <div class="col-md-10">
      <h2>Sail (Zifan) Wang</h2>

      Ph.D. in Electrical and Computer Engineering<br/>
      Carnegie Mellon University, Pittsburgh, PA, U.S<br/><br/>
      Email: zifan.wang@sv.cmu.edu </a><br/>
      Cubic: 2205, CIC </a><br/>
    </div>

  </div>
  </div>
</div>

<div class="container">


    <div class="row">
      <div class="col-md-12">
<h2>About</h2>

	<p>
  Iâ€™m a first-year Ph.D. student in Electrical and Computer Enigneering, Carnegie Mellon University (CMU).
	I am co-advised by Prof. <a href="http://www.andrew.cmu.edu/user/danupam/">Anupam Datta</a> and Prof. <a href="http://www.cs.cmu.edu/~mfredrik/">Matt Fredrikson</a>
	</p>

	<p>
	I am a member of <a href="https://fairlyaccountable.org">Accountable System Lab</a> and My current concentration is the development of interpretations for deep neural networks.
  </p>
  
  <p>
    I will be receiving my Master degree in Electrical and Computer Enigneering in the comming May, 2020. During my Master program, I study at the Silicon Valley campus of CMU, a small and warm community located in Mountain View, CA. Before joining CMU, I received my Bachelor degree in Electronic Science and Technology from Beijing Institute of Technology, Beijing, China.
  </p>

  <p>
    Outside my professional life, I am an outgoing vidoe game player, a hiker who also loves camping and road trip, and right now I am learning to play the skateboard. I also have a cat named with Pikachu. He is handsome and easy-going. 

  </p>


<h2>Publications</h2>

	<ul>
	  <li>
<p><a href="https://arxiv.org/abs/2002.07985"><b>Interpreting Interpretations: Organizing Attribution Methods by Criteria</b></a>.&nbsp;Zifan Wang, Piotr Mardziel, Anupam Datta, Matt Fredrikson
   <i>[CVPR 2020 Workshop]</i><br/>
   [ 
     <a href="https://arxiv.org/abs/2002.07985">pdf</a>
     | <a href="javascript:toggle('bibcrftut:fnt', 'bib_link_crftut:fnt', 'bib')"  id="bib_link_crftut:fnt">bib</a>
   ]  
   <p>Motivated by distinct, though related, criteria, a growing number of attribution methods have been developed tointerprete deep learning. While each relies on the interpretability of the concept of "importance" and our ability to visualize patterns, explanations produced by the methods often differ. As a result, input attribution for vision models fail to provide any level of human understanding of model behaviour. In this work we expand the foundationsof human-understandable concepts with which attributionscan be interpreted beyond "importance" and its visualization; we incorporate the logical concepts of necessity andsufficiency, and the concept of proportionality. We definemetrics to represent these concepts as quantitative aspectsof an attribution. This allows us to compare attributionsproduced by different methods and interpret them in novelways: to what extent does this attribution (or this method)represent the necessity or sufficiency of the highlighted inputs, and to what extent is it proportional? We evaluate our measures on a collection of methods explaining convolutional neural networks (CNN) for image classification. We conclude that some attribution methods are more appropriate for interpretation in terms of necessity while others are in terms of sufficiency, while no method is always the most appropriate in terms of both.</p>
</p>

<div style="display:none;" id="bibcrftut:fnt"><pre class="bibtex">@misc{wang2020interpreting,
  title={Interpreting Interpretations: Organizing Attribution Methods by Criteria},
  author={Zifan Wang and Piotr Mardziel and Anupam Datta and Matt Fredrikson},
  year={2020},
  eprint={2002.07985},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}
</pre></div>
</li>
</ul>

<ul>
  <li>
<p><a href="https://arxiv.org/abs/1910.01279"><b>Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks</b></a>.&nbsp;Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu
 <i>[CVPR 2020 Workshop]</i><br/>
 [ 
   <a href="https://arxiv.org/abs/1910.01279">pdf</a>
   | <a href="javascript:toggle('bibcrftut2:fnt', 'bib_link_crftut2:fnt', 'bib')"  id="bib_link_crftut2:fnt">bib</a>
 ]  
 <p>Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. Official code has been released.</p>
</p>

<div style="display:none;" id="bibcrftut2:fnt"><pre class="bibtex">@misc{wang2019scorecam,
  title={Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
  author={Haofan Wang and Zifan Wang and Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
  year={2019},
  eprint={1910.01279},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
</pre></div>
</li>
</ul>

<!-- <h2>Pre-print</h2>
<ul>
  <li>
<p><a href="https://arxiv.org/abs/1910.01279"><b>Towards Frequency-Based Explanation for Robust CNN </b></a>.&nbsp;Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal and Zihao Ding
 <i></i><br/>
 [ 
   <a href="publications/here-is-my-paper.pdf">pdf</a>
   | <a href="javascript:toggle('bibcrftut:fnt', 'bib_link_crftut:fnt', 'bib')"  id="bib_link_crftut:fnt">bib</a>
 ]  
 <p>Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions.</p>
</p>

<div style="display:none;" id="bibcrftut:fnt"><pre class="bibtex">@misc{wang2019scorecam,
  title={Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
  author={Haofan Wang and Zifan Wang and Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
  year={2019},
  eprint={1910.01279},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
</pre></div>
</li>
</ul> -->

<h2>Teaching</h2>
<li>[Teaching Assistance] 2019 Fall 18-661: Introduction to Machine Learning for Engineers <a href="https://18661.github.io/">course homepage</a></li> 
<li>[Teaching Assistance] 2020 Spring 18-739: Security and Fairness of Deep Learning <a href="http://www.archive.ece.cmu.edu/~ece739/index.html">course homepage</a></li> 

<h2>Gateway</h2>

<ul>
  <li>My homepage on <a href="https://www.linkedin.com/in/zifan-wang-sail/">LinkedIn</a></li> 
  <li>Follow me on <a href="https://github.com/SailColubrid">Github</a></li> 

</ul>

<h2>My Latest Travel Picture</h2>
<img class="photo" src="images/travel.jpg" alt="Photo">
<p>  </p>
<img class="photo" src="images/travel2.jpg" alt="Photo">


</div>
</div>
</div>

    
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" type="text/javascript"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/csutton/js/bootstrap.min.js" type="text/javascript"></script>
  </body>
</html>
