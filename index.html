<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <title>Zifan Wang</title>

  <!-- Bootstrap -->
  <link href="css/bootstrap.min.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

  <script src="js/main.js" type="text/javascript"></script>

  <!-- Fonts -->
  <!-- ADD A GOOGLE FONT HERE IF YOU LIKE -->


</head>

<body>


  <div class="color-header">

    <div class="container page-header">

      <div class="row" id="contact">
        <div class="col-md-2">
          <img class="photo" src="images/photo.jpg" alt="Photo">
        </div>
        <div class="col-md-10">
          <h2>Sail (Zifan) Wang</h2>

          Ph.D. in Electrical and Computer Engineering<br />
          Carnegie Mellon University, Pittsburgh, PA, U.S<br /><br />
          Email: zifan.wang@sv.cmu.edu </a><br />
          Cubic: 2205, CIC </a><br />
        </div>

      </div>
    </div>
  </div>

  <div class="container">


    <div class="row">
      <div class="col-md-12">
        <h2>About</h2>

        <p>
          I’m a first-year Ph.D. student in Electrical and Computer Enigneering, Carnegie Mellon University (CMU).
          I am co-advised by Prof. <a href="http://www.andrew.cmu.edu/user/danupam/">Anupam Datta</a> and Prof. <a
            href="http://www.cs.cmu.edu/~mfredrik/">Matt Fredrikson</a>
        </p>

        <p>
          I am a member of <a href="https://fairlyaccountable.org">Accountable System Lab</a> and My current
          concentration is the development of interpretations for deep neural networks.
        </p>

        <p>
          I will be receiving my Master degree in Electrical and Computer Enigneering in the comming May, 2020. During
          my Master program, I study at the Silicon Valley campus of CMU, a small and warm community located in Mountain
          View, CA. Before joining CMU, I received my Bachelor degree in Electronic Science and Technology from Beijing
          Institute of Technology, Beijing, China.
        </p>

        <p>
          Outside my professional life, I am an outgoing video game player, a hiker who also loves camping and road
          trip, and right now I am learning to play the skateboard. I also have a cat whose name is Pikachu. He is
          handsome and active

        </p>


        <h2>Publications</h2>


        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/2005.03141"><b>[NuerIPS 2020] Smoothed Geometry for Robust Attribution</b></a>.&nbsp; Zifan Wang, Haofan Wang, Shakul Ramkumar, Matt Fredrikson, Piotr Mardziel, Anupam Datta
              <br />
              [
              <a href="https://arxiv.org/pdf/2006.06643v1.pdf">pdf</a>
              | <a href="javascript:toggle('bibcrftut4:fnt', 'bib_link_crftut4:fnt', 'bib')"
                id="bib_link_crftut3:fnt">bib</a>
              ]
            <p>Feature attributions are a popular tool for explaining the behavior of Deep Neural Networks (DNNs), but have recently been shown to be vulnerable to attacks that produce divergent explanations for nearby inputs. This lack of robustness is especially problematic in high-stakes applications where adversarially-manipulated explanations could impair safety and trustworthiness. Building on a geometric understanding of these attacks presented in recent work, we identify Lipschitz continuity conditions on models' gradient that lead to robust gradient-based attributions, and observe that smoothness may also be related to the ability of an attack to transfer across multiple attribution methods. To mitigate these attacks in practice, we propose an inexpensive regularization method that promotes these conditions in DNNs, as well as a stochastic smoothing technique that does not require re-training. Our experiments on a range of image models demonstrate that both of these mitigations consistently improve attribution robustness, and confirm the role that smooth geometry plays in these attacks on real, large-scale models.</p>
            </p>

            <div style="display:none;" id="bibcrftut4:fnt">
              <pre class="bibtex">@misc{wang2020smoothed,
                title={Smoothed Geometry for Robust Attribution},
                author={Zifan Wang and Haofan Wang and Shakul Ramkumar and Matt Fredrikson and Piotr Mardziel and Anupam Datta},
                year={2020},
                eprint={2006.06643},
                archivePrefix={arXiv},
                primaryClass={cs.LG}
            }
</pre>
            </div>
          </li>
        </ul>


        <ul>
          <li>
            <p><a
                href="http://openaccess.thecvf.com/content_CVPRW_2020/papers/w1/Wang_Interpreting_Interpretations_Organizing_Attribution_Methods_by_Criteria_CVPRW_2020_paper.pdf"><b>[CVPR 2020 Workshop] Interpreting
                  Interpretations: Organizing Attribution
                  Methods by Criteria</b></a>.&nbsp;Zifan Wang, Piotr Mardziel, Anupam Datta, Matt Fredrikson
              <i>[</i><br />
              [
              <a href="https://arxiv.org/abs/2002.07985">arXiv</a>
              | <a href="javascript:toggle('bibcrftut:fnt', 'bib_link_crftut:fnt', 'bib')"
                id="bib_link_crftut:fnt">bib</a>
              | <a href="./resource/IIOAMC/slides.pdf">slides</a>
              | <a href="https://youtu.be/cyYHoTyNMfQ">video</a>
              ]
            <p>Motivated by distinct, though related, criteria, a growing number of attribution methods have been
              developed tointerprete deep learning. While each relies on the interpretability of the concept of
              "importance" and our ability to visualize patterns, explanations produced by the methods often differ.
              As a result, input attribution for vision models fail to provide any level of human understanding of
              model behaviour. In this work we expand the foundationsof human-understandable concepts with which
              attributionscan be interpreted beyond "importance" and its visualization; we incorporate the logical
              concepts of necessity andsufficiency, and the concept of proportionality. We definemetrics to represent
              these concepts as quantitative aspectsof an attribution. This allows us to compare attributionsproduced
              by different methods and interpret them in novelways: to what extent does this attribution (or this
              method)represent the necessity or sufficiency of the highlighted inputs, and to what extent is it
              proportional? We evaluate our measures on a collection of methods explaining convolutional neural
              networks (CNN) for image classification. We conclude that some attribution methods are more appropriate
              for interpretation in terms of necessity while others are in terms of sufficiency, while no method is
              always the most appropriate in terms of both.</p>
            </p>

            <div style="display:none;" id="bibcrftut:fnt">
              <pre class="bibtex">@InProceedings{Wang_2020_CVPR_Workshops,
                author = {Wang, Zifan and Mardziel, Piotr and Datta, Anupam and Fredrikson, Matt},
                title = {Interpreting Interpretations: Organizing Attribution Methods by Criteria},
                booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
                month = {June},
                year = {2020}
                }
</pre>
            </div>
          </li>
        </ul>

        <ul>
          <li>
            <p><a
                href="http://openaccess.thecvf.com/content_CVPRW_2020/papers/w1/Wang_Score-CAM_Score-Weighted_Visual_Explanations_for_Convolutional_Neural_Networks_CVPRW_2020_paper.pdf"><b>[CVPR 2020 Workshop] Score-CAM:
                  Score-Weighted Visual Explanations for
                  Convolutional Neural Networks</b></a>.&nbsp;Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian
              Zhang, Sirui Ding, Piotr Mardziel, Xia Hu
              <i></i><br />
              [
              <a href="https://arxiv.org/abs/1910.01279">arXiv</a>
              | <a href="javascript:toggle('bibcrftut2:fnt', 'bib_link_crftut2:fnt', 'bib')"
                id="bib_link_crftut2:fnt">bib</a>
              | <a href="./resource/Score_CAM/slides.pdf">slides</a>
              ]
            <p>Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural
              networks, and the reason why the network makes specific decisions. In this paper, we develop a novel
              post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous
              class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by
              obtaining the weight of each activation map through its forward passing score on target class, the final
              result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM
              achieves better visual performance and fairness for interpreting the decision making process. Our
              approach outperforms previous methods on both recognition and localization tasks, it also passes the
              sanity check. We also indicate its application as debugging tools. Official code has been released.</p>
            </p>

            <div style="display:none;" id="bibcrftut2:fnt">
              <pre class="bibtex">@InProceedings{Wang_2020_CVPR_Workshops,
                author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
                title = {Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
                booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
                month = {June},
                year = {2020}
                }
</pre>
            </div>
          </li>
        </ul>


        <!-- <h2>Pre-print</h2>
<ul>
  <li>
<p><a href="https://arxiv.org/abs/1910.01279"><b>Towards Frequency-Based Explanation for Robust CNN </b></a>.&nbsp;Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal and Zihao Ding
 <i></i><br/>
 [ 
   <a href="publications/here-is-my-paper.pdf">pdf</a>
   | <a href="javascript:toggle('bibcrftut:fnt', 'bib_link_crftut:fnt', 'bib')"  id="bib_link_crftut:fnt">bib</a>
 ]  
 <p>Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions.</p>
</p>

<div style="display:none;" id="bibcrftut:fnt"><pre class="bibtex">@misc{wang2019scorecam,
  title={Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
  author={Haofan Wang and Zifan Wang and Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
  year={2019},
  eprint={1910.01279},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
</pre></div>
</li>
</ul> -->

        <h2>Pre-print</h2>
        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/2005.03141"><b>Towards Frequency-Based Explanation for Robust
                  CNN</b></a>.&nbsp;Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, Zihao Ding
              <i></i><br />
              [
              <a href="https://arxiv.org/abs/2005.03141">pdf</a>
              | <a href="javascript:toggle('bibcrftut3:fnt', 'bib_link_crftut3:fnt', 'bib')"
                id="bib_link_crftut3:fnt">bib</a>
              ]
            <p>Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses
              on building connections between the human-understandable input features with models' prediction,
              overlooking an alternative representation of the input, the frequency components decomposition. In this
              work, we present an analysis of the connection between the distribution of frequency components in the
              input dataset and the reasoning process the model learns from the data. We further provide
              quantification analysis about the contribution of different frequency components toward the model's
              prediction. We show that the vulnerability of the model against tiny distortions is a result of the
              model is relying on the high-frequency features, the target features of the adversarial (black and
              white-box) attackers, to make the prediction. We further show that if the model develops stronger
              association between the low-frequency component with true labels, the model is more robust, which is the
              explanation of why adversarially trained models are more robust against tiny distortions.</p>
            </p>

            <div style="display:none;" id="bibcrftut3:fnt">
              <pre class="bibtex">@misc{wang2020frequencybased,
  title={Towards Frequency-Based Explanation for Robust CNN},
  author={Zifan Wang and Yilin Yang and Ankit Shrivastava and Varun Rawal and Zihao Ding},
  year={2020},
  eprint={2005.03141},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
</pre>
            </div>
          </li>
        </ul>


        <ul>
          <li>
            <p><a href="https://arxiv.org/abs/2009.08507"><b>Towards Behavior-Level Explanation for Deep Reinforcement Learning</b></a>.&nbsp; Xuan Chen, Zifan Wang, Yucai Fan, Bonan Jin, Piotr Mardziel, Carlee Joe-Wong, Anupam Datta
              <i></i><br />
              [
              <a href="https://arxiv.org/pdf/2009.08507.pdf">pdf</a>
              | <a href="javascript:toggle('bibcrftut4:fntff', 'bib_link_crftut4:fntff', 'bib')"
                id="bib_link_crftut3:fnt">bib</a>
              ]
            <p>While Deep Neural Networks (DNNs) are becoming the state-of-the-art for many tasks including reinforce- ment learning (RL), they are especially resistant to hu- man scrutiny and understanding. Input attributions have been a foundational building block for DNN expalainabilty but face new challenges when applied to deep RL. We ad- dress the challenges with two novel techniques. We de- fine a class of behaviour-level attributions for explaining agent behaviour beyond input importance and interpret ex- isting attribution methods on the behaviour level. We then introduce λ-alignment, a metric for evaluating the perfors- mance of behaviour-level attributions methods in terms of whether they are indicative of the agent actions they are meant to explain. Our experiments on Atari games sug- gest that perturbation-based attribution methods are sig- nificantly more suitable to deep RL than alternatives from the perspective of this metric. We argue that our methods demonstrate the minimal set of considerations for adopting general DNN explanation technology to the unique aspects of reinforcement learning and hope the outlined direction can serve as a basis for future research on understanding Deep RL using attribution.</p>
            </p>

            <div style="display:none;" id="bibcrftut4:fntff">
              <pre class="bibtex">@misc{chen2020behaviorlevel,
                title={Towards Behavior-Level Explanation for Deep Reinforcement Learning},
                author={Xuan Chen and Zifan Wang and Yucai Fan and Bonan Jin and Piotr Mardziel and Carlee Joe-Wong and Anupam Datta},
                year={2020},
                eprint={2009.08507},
                archivePrefix={arXiv},
                primaryClass={cs.AI}
            }
</pre>
            </div>
          </li>
        </ul>

        <h2>Teaching</h2>
        <li>[Teaching Assistance] 2019 Fall 18-661: Introduction to Machine Learning for Engineers <a
            href="https://18661.github.io/">course homepage</a></li>
        <li>[Teaching Assistance] 2020 Spring 18-739: Security and Fairness of Deep Learning <a
            href="http://www.archive.ece.cmu.edu/~ece739/index.html">course homepage</a></li>

        <h2>News</h2>
        <li>[2020-05-16] 言聚 Talk <a href="https://mp.weixin.qq.com/s/sjHVnp7sRN0eMf9c-1DDjQ">强大却无法理解的AI, 该不该使用？</a> </a>
        </li>


        <h2>Gateway</h2>

        <ul>
          <li>My homepage on <a href="https://www.linkedin.com/in/zifan-wang-sail/">LinkedIn</a></li>
          <li>Follow me on <a href="https://github.com/SailColubrid">Github</a></li>

        </ul>

        <h2>My Latest Travel Picture</h2>
        <img class="photo" src="images/travel.jpg" alt="Photo">
        <p> </p>
        <img class="photo" src="images/travel2.jpg" alt="Photo">


      </div>
    </div>
  </div>


  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js" type="text/javascript"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/csutton/js/bootstrap.min.js" type="text/javascript"></script>
</body>

</html>